{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "mount_file_id": "12onpi02EFHPdo36JqAiRhjeqU6Qsuv80",
      "authorship_tag": "ABX9TyPcQ5M8ZaKpf7NdOi/JKWGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiza-daiki/TreeSpeciesClassification/blob/main/TreeSpeciesDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVpfyT_Gbjej"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6b0196"
      },
      "source": [
        "# Task\n",
        "PyTorchを使用して、提示されたファイル構成に基づき、樹皮画像を用いた樹種判別を行う画像認識の学習器を構築する。将来的なクラス増分（増分学習・知識蒸留）を視野に入れ、まずはStage 1のデータで学習・評価を行い、その後Stage 2のデータで増分学習を行い、最後に全てのクラスを含むデータで総合評価を行う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9287be29"
      },
      "source": [
        "## 環境構築\n",
        "\n",
        "### Subtask:\n",
        "PyTorch および必要なライブラリをインストールします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4619a82"
      },
      "source": [
        "**Reasoning**:\n",
        "Install PyTorch and other necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c154ec31",
        "outputId": "31fc371d-f3a7-44ad-c82d-cc5380de8448"
      },
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install Pillow opencv-python pandas numpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (12.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26d57190"
      },
      "source": [
        "## データセットクラスとデータローダーの定義\n",
        "\n",
        "### Subtask:\n",
        "提示されたファイル構成に合わせて、各ステージのデータをロードするためのカスタムデータセットクラスとデータローダーを定義します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4e2f8e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the custom dataset class and data loaders as per the instructions, including loading images, applying transformations, and handling labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "74fb2a4f",
        "outputId": "8293d885-73a2-478a-baef-555704aba442"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class BarkDataset(Dataset):\n",
        "    def __init__(self, base_dir, stage, split, transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.stage = stage\n",
        "        self.split = split # 'train', 'val', or 'test'\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        self.classes = sorted(os.listdir(os.path.join(base_dir, f'stage_{stage}', split)))\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(base_dir, f'stage_{stage}', split, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                self.image_files.append(os.path.join(class_dir, img_name))\n",
        "                self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Assuming a base directory structure like:\n",
        "# base_dir/\n",
        "#   stage_1/\n",
        "#     train/\n",
        "#       class_a/\n",
        "#       class_b/\n",
        "#     val/\n",
        "#       class_a/\n",
        "#       class_b/\n",
        "#     test/\n",
        "#       class_a/\n",
        "#       class_b/\n",
        "#   stage_2/\n",
        "#     train/\n",
        "#       class_c/\n",
        "#       class_d/\n",
        "#     val/\n",
        "#       class_c/\n",
        "#       class_d/\n",
        "#     test/\n",
        "#       class_c/\n",
        "#       class_d/\n",
        "#   all_classes_test/ # For final evaluation\n",
        "#     class_a/\n",
        "#     class_b/\n",
        "#     class_c/\n",
        "#     class_d/\n",
        "\n",
        "\n",
        "# Define data loaders for each stage and split\n",
        "# Note: Replace 'path/to/your/data' with the actual base directory\n",
        "base_data_dir = '/content/sample_data' # Placeholder, replace with actual path\n",
        "\n",
        "# Stage 1\n",
        "stage1_train_dataset = BarkDataset(base_data_dir, stage=1, split='train', transform=transform)\n",
        "stage1_val_dataset = BarkDataset(base_data_dir, stage=1, split='val', transform=transform)\n",
        "stage1_test_dataset = BarkDataset(base_data_dir, stage=1, split='test', transform=transform)\n",
        "\n",
        "stage1_train_loader = DataLoader(stage1_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "stage1_val_loader = DataLoader(stage1_val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "stage1_test_loader = DataLoader(stage1_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Stage 2\n",
        "stage2_train_dataset = BarkDataset(base_data_dir, stage=2, split='train', transform=transform)\n",
        "stage2_val_dataset = BarkDataset(base_data_dir, stage=2, split='val', transform=transform)\n",
        "stage2_test_dataset = BarkDataset(base_data_dir, stage=2, split='test', transform=transform)\n",
        "\n",
        "stage2_train_loader = DataLoader(stage2_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "stage2_val_loader = DataLoader(stage2_val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "stage2_test_loader = DataLoader(stage2_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# All classes test data (assuming a separate directory for this)\n",
        "# This part assumes a different directory structure for the final test set,\n",
        "# where all classes are present in a single 'all_classes_test' directory.\n",
        "# You might need to adjust the BarkDataset class or create a new one\n",
        "# if your 'all_classes_test' structure is different.\n",
        "# For simplicity, let's assume a structure similar to stage_1/test or stage_2/test\n",
        "# but with all classes. A slight modification to BarkDataset might be needed\n",
        "# to handle a directory structure like base_dir/all_classes_test/class_name/image.jpg\n",
        "\n",
        "# To handle the 'all_classes_test' scenario, we can reuse BarkDataset but point\n",
        "# it to the 'all_classes_test' directory and adjust how classes are discovered.\n",
        "# We'll assume 'all_classes_test' is structured like stage directories.\n",
        "\n",
        "class AllClassesTestDataset(Dataset):\n",
        "    def __init__(self, base_dir, transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        # Assuming all_classes_test has class subdirectories directly under it\n",
        "        self.classes = sorted(os.listdir(os.path.join(base_dir, 'all_classes_test')))\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(base_dir, 'all_classes_test', class_name)\n",
        "            if os.path.isdir(class_dir): # Ensure it's a directory\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    self.image_files.append(os.path.join(class_dir, img_name))\n",
        "                    self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "all_classes_test_dataset = AllClassesTestDataset(base_data_dir, transform=transform)\n",
        "all_classes_test_loader = DataLoader(all_classes_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Custom Dataset and DataLoaders defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/stage_1/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3033939690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Stage 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mstage1_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarkDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mstage1_val_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarkDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mstage1_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarkDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3033939690.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_dir, stage, split, transform)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'stage_{stage}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/stage_1/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "514fd115"
      },
      "source": [
        "## ベースモデルの定義\n",
        "\n",
        "### Subtask:\n",
        "樹種判別を行うためのベースとなるモデル（CNNなど）をPyTorchで定義します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea9dc126"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a simple Convolutional Neural Network (CNN) model using `torch.nn.Module` for image classification, considering the input image size and the number of classes from Stage 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabcb17f",
        "outputId": "d628fb15-ee70-4460-a791-bc0136b9aea3"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Assuming input image size is 224x224 after transformations\n",
        "        # Calculate the size of the flattened features after pooling\n",
        "        # 224 / 2 / 2 / 2 = 28\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.pool3(self.relu3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1) # Flatten the output for the fully connected layers\n",
        "        x = self.dropout(self.relu4(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Determine the number of classes for Stage 1.\n",
        "# This requires the stage1_train_dataset to be successfully instantiated,\n",
        "# which failed in the previous step due to missing data.\n",
        "# We will use a placeholder value for now, assuming Stage 1 has 2 classes based on the previous step's assumption of 'class_a' and 'class_b'.\n",
        "# In a real scenario, this would be determined from the dataset object: stage1_train_dataset.num_classes\n",
        "\n",
        "num_classes_stage1 = 2 # Placeholder - replace with actual number of classes in Stage 1\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleCNN(num_classes=num_classes_stage1)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model defined and moved to {device}.\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model defined and moved to cpu.\n",
            "SimpleCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu3): ReLU()\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
            "  (relu4): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "765b55b6"
      },
      "source": [
        "## ステージ1での学習\n",
        "\n",
        "### Subtask:\n",
        "Stage 1のデータ (`stage_1/train`) を使用してベースモデルを学習させます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46395dfe"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the loss function and optimizer, set up the training and validation loop, and train the model on the stage 1 data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "82bbbe1f",
        "outputId": "88c66cb0-1fb8-418f-f7bb-62aeff74f186"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Set up training parameters\n",
        "num_epochs = 10\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training loop\n",
        "    for i, data in enumerate(stage1_train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(stage1_train_loader)\n",
        "    epoch_accuracy = 100 * correct_train / total_train\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in stage1_val_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(stage1_val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), 'best_stage1_model.pth')\n",
        "        print(\"Saved best model state dictionary.\")\n",
        "\n",
        "print(\"Finished Training.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stage1_train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3306633538.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage1_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stage1_train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6503ef07"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `stage1_train_loader` is not defined. This is because the previous step where the data loaders were defined failed due to missing data files. Although the definition itself was conceptually correct, the instantiation failed. To proceed with training, we need to re-execute the data loader definition step. Since the goal is to train the model, and the model definition was successful, we will redefine the data loaders and then proceed with the training loop in a single code block. We will include the necessary imports from the previous successful steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOO7S5UZdbvD",
        "outputId": "f751d261-529b-49a1-e2de-7df5e0f547c1"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# Redefine the BarkDataset class and DataLoaders (as the previous attempt failed)\n",
        "class BarkDataset(Dataset):\n",
        "    def __init__(self, base_dir, stage, split, transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.stage = stage\n",
        "        self.split = split # 'train', 'val', or 'test'\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        # Adjusted to handle potential missing directories more gracefully, though it will still error if base_dir is wrong\n",
        "        stage_split_dir = os.path.join(base_dir, f'stage_{stage}', split)\n",
        "        if not os.path.exists(stage_split_dir):\n",
        "             raise FileNotFoundError(f\"Directory not found: {stage_split_dir}\")\n",
        "\n",
        "        self.classes = sorted([d for d in os.listdir(stage_split_dir) if os.path.isdir(os.path.join(stage_split_dir, d))])\n",
        "        if not self.classes:\n",
        "             raise ValueError(f\"No class directories found in {stage_split_dir}\")\n",
        "\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(stage_split_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                self.image_files.append(os.path.join(class_dir, img_name))\n",
        "                self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, None # Handle potential image loading errors\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Assuming a base directory structure like:\n",
        "# base_dir/\n",
        "#   stage_1/\n",
        "#     train/\n",
        "#       class_a/\n",
        "#       class_b/\n",
        "#     val/\n",
        "#       class_a/\n",
        "#       class_b/\n",
        "#     test/\n",
        "#       class_a/\n",
        "#       class_b/\n",
        "#   stage_2/\n",
        "#     train/\n",
        "#       class_c/\n",
        "#       class_d/\n",
        "#     val/\n",
        "#       class_c/\n",
        "#       class_d/\n",
        "#     test/\n",
        "#       class_c/\n",
        "#       class_d/\n",
        "#   all_classes_test/ # For final evaluation\n",
        "#     class_a/\n",
        "#     class_b/\n",
        "#     class_c/\n",
        "#     class_d/\n",
        "\n",
        "\n",
        "# Define data loaders for each stage and split\n",
        "# Note: Replace 'path/to/your/data' with the actual base directory\n",
        "# Using a placeholder again, as the actual data path is unknown. This will likely fail again\n",
        "# if the data is not present, but it's necessary to attempt to define the loaders\n",
        "# before the training loop.\n",
        "base_data_dir = '/content/sample_data' # Placeholder, replace with actual path\n",
        "\n",
        "try:\n",
        "    # Stage 1\n",
        "    stage1_train_dataset = BarkDataset(base_data_dir, stage=1, split='train', transform=transform)\n",
        "    stage1_val_dataset = BarkDataset(base_data_dir, stage=1, split='val', transform=transform)\n",
        "    stage1_test_dataset = BarkDataset(base_data_dir, stage=1, split='test', transform=transform)\n",
        "\n",
        "    stage1_train_loader = DataLoader(stage1_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    stage1_val_loader = DataLoader(stage1_val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    stage1_test_loader = DataLoader(stage1_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Stage 2 (defined for completeness, though not used in this subtask)\n",
        "    stage2_train_dataset = BarkDataset(base_data_dir, stage=2, split='train', transform=transform)\n",
        "    stage2_val_dataset = BarkDataset(base_data_dir, stage=2, split='val', transform=transform)\n",
        "    stage2_test_dataset = BarkDataset(base_data_dir, stage=2, split='test', transform=transform)\n",
        "\n",
        "    stage2_train_loader = DataLoader(stage2_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    stage2_val_loader = DataLoader(stage2_val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    stage2_test_loader = DataLoader(stage2_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    # All classes test data (defined for completeness, though not used in this subtask)\n",
        "    class AllClassesTestDataset(Dataset):\n",
        "        def __init__(self, base_dir, transform=None):\n",
        "            self.base_dir = base_dir\n",
        "            self.transform = transform\n",
        "            self.image_files = []\n",
        "            self.labels = []\n",
        "            all_classes_dir = os.path.join(base_dir, 'all_classes_test')\n",
        "            if not os.path.exists(all_classes_dir):\n",
        "                 raise FileNotFoundError(f\"Directory not found: {all_classes_dir}\")\n",
        "\n",
        "            self.classes = sorted([d for d in os.listdir(all_classes_dir) if os.path.isdir(os.path.join(all_classes_dir, d))])\n",
        "            if not self.classes:\n",
        "                raise ValueError(f\"No class directories found in {all_classes_dir}\")\n",
        "\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "            for class_name in self.classes:\n",
        "                class_dir = os.path.join(all_classes_dir, class_name)\n",
        "                if os.path.isdir(class_dir): # Ensure it's a directory\n",
        "                    for img_name in os.listdir(class_dir):\n",
        "                        self.image_files.append(os.path.join(class_dir, img_name))\n",
        "                        self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.image_files)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img_path = self.image_files[idx]\n",
        "            try:\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                return None, None # Handle potential image loading errors\n",
        "\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "    all_classes_test_dataset = AllClassesTestDataset(base_data_dir, transform=transform)\n",
        "    all_classes_test_loader = DataLoader(all_classes_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"Custom Dataset and DataLoaders defined.\")\n",
        "\n",
        "    # Define loss function and optimizer (re-defined for clarity in this block)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Set up training parameters (re-defined for clarity)\n",
        "    num_epochs = 10\n",
        "    best_val_accuracy = 0.0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # Training loop\n",
        "        for i, data in enumerate(stage1_train_loader):\n",
        "            inputs, labels = data\n",
        "            # Filter out None values in case of image loading errors\n",
        "            if inputs is None or labels is None:\n",
        "                continue\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(stage1_train_loader)\n",
        "        epoch_accuracy = 100 * correct_train / total_train\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in stage1_val_loader:\n",
        "                images, labels = data\n",
        "                # Filter out None values\n",
        "                if images is None or labels is None:\n",
        "                    continue\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(stage1_val_loader)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save the model if validation accuracy improves\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_stage1_model.pth')\n",
        "            print(\"Saved best model state dictionary.\")\n",
        "\n",
        "    print(\"Finished Training.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}. Please ensure the data directory structure is correct.\")\n",
        "    # The task cannot be completed without the data loaders being successfully instantiated.\n",
        "    # We'll need to signal failure or request the user to provide the correct path/data.\n",
        "    # As per instructions, we should not ask for user input.\n",
        "    # We will finish the task with failure because the prerequisite (data loading) failed.\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}. Please ensure there are class subdirectories in your data folders.\")\n",
        "    # Similar to FileNotFoundError, cannot proceed without valid data.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    # Catch any other unexpected errors during data loading or training.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Directory not found: /content/sample_data/stage_1/train. Please ensure the data directory structure is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0f34af4"
      },
      "source": [
        "データセットクラス `BarkDataset` および `AllClassesTestDataset` の定義に基づくと、データは以下のファイル構成になっている必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9f681ff"
      },
      "source": [
        "## ステージ1での評価\n",
        "\n",
        "### Subtask:\n",
        "学習済みモデルをStage 1のテストデータ (`stage_1/test`) で評価し、初期性能を確認します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a5aeb1b"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model on the Stage 1 test data by setting the model to evaluation mode, iterating through the test loader, making predictions, and calculating accuracy and a confusion matrix. This requires the model and the test loader to be available from previous steps. Since the training step failed due to data loading issues, this step will also likely fail unless the data path is corrected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2230155",
        "outputId": "01f44252-a4a1-4654-a90f-e9a897224447"
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Check if model and stage1_test_loader exist from previous steps\n",
        "# If the previous training step failed due to data loading, these might not be defined.\n",
        "# We will assume they exist based on the flow, but the code will fail if they don't.\n",
        "\n",
        "if 'model' not in locals() or 'stage1_test_loader' not in locals():\n",
        "    print(\"Error: Model or stage1_test_loader not found. Ensure previous steps (defining model and data loaders) were successful.\")\n",
        "else:\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    print(\"Starting evaluation on Stage 1 test data...\")\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    with torch.no_grad():\n",
        "        for data in stage1_test_loader:\n",
        "            images, labels = data\n",
        "            # Filter out None values in case of image loading errors\n",
        "            if images is None or labels is None:\n",
        "                continue\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    print(f'Stage 1 Test Accuracy: {accuracy:.4f}')\n",
        "    print('Stage 1 Confusion Matrix:')\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Determine class names if possible (assuming stage1_test_dataset was created successfully)\n",
        "    if 'stage1_test_dataset' in locals():\n",
        "        print('Stage 1 Class Names:')\n",
        "        print(stage1_test_dataset.classes)\n",
        "\n",
        "print(\"Finished evaluation.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Model or stage1_test_loader not found. Ensure previous steps (defining model and data loaders) were successful.\n",
            "Finished evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37cae921"
      },
      "source": [
        "## 増分学習の準備\n",
        "\n",
        "### Subtask:\n",
        "Stage 2のデータ (`stage_2/train`) を使用して増分学習を行うための準備をします。必要に応じて、知識蒸留のための設定も検討します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64579529"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps failed because the data loaders could not be instantiated due to a `FileNotFoundError` and `ValueError` related to the data directory structure. Before proceeding with Stage 2 incremental learning, it is crucial to ensure that the Stage 2 data loader (`stage2_train_loader`) can be successfully created. This involves verifying and potentially correcting the `base_data_dir` and the `BarkDataset` definition's ability to locate Stage 2 training data. I will reuse the `BarkDataset` and `DataLoader` definitions from the previous attempts and attempt to instantiate the `stage2_train_dataset` and `stage2_train_loader` again. I will also check if the `model` object exists from the previous steps, as it is needed for incremental learning. If the data loading is successful, I will set up the teacher model for knowledge distillation and define a new optimizer and loss function as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "547a44e5",
        "outputId": "bf4bfb2b-b57f-4250-bf01-d3b92a9cd43e"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F # For KL Divergence\n",
        "\n",
        "# Re-verify the base_data_dir and attempt to instantiate Stage 2 data loader\n",
        "# The BarkDataset and DataLoader classes are assumed to be defined in a previous cell.\n",
        "# If that cell failed, this will fail again.\n",
        "\n",
        "# Using the placeholder base_data_dir again. This needs to be correct for the code to run.\n",
        "base_data_dir = '/content/sample_data' # Placeholder, replace with actual path\n",
        "\n",
        "try:\n",
        "    # Attempt to instantiate Stage 2 train dataset and loader\n",
        "    stage2_train_dataset = BarkDataset(base_data_dir, stage=2, split='train', transform=transform)\n",
        "    stage2_train_loader = DataLoader(stage2_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    print(\"Stage 2 train dataset and loader successfully created.\")\n",
        "\n",
        "    # Check if the model exists from previous steps\n",
        "    if 'model' in locals():\n",
        "        print(\"Model instance found.\")\n",
        "        # 3. Set Stage 1 model as teacher model and switch to evaluation mode\n",
        "        teacher_model = model\n",
        "        teacher_model.eval()\n",
        "        print(\"Stage 1 model set as teacher model and set to evaluation mode.\")\n",
        "\n",
        "        # 4. Define a new optimizer for incremental learning (Stage 2)\n",
        "        # Using Adam with a potentially different learning rate.\n",
        "        incremental_optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "        print(\"New optimizer for incremental learning defined.\")\n",
        "\n",
        "        # 5. Define loss function(s)\n",
        "        # Cross-entropy loss for the student model\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        print(\"Cross-entropy loss defined.\")\n",
        "\n",
        "        # If knowledge distillation is used, define KL divergence loss\n",
        "        # Assuming knowledge distillation will be implemented in the training loop.\n",
        "        # KL divergence loss will be used to match student output probabilities to teacher output probabilities.\n",
        "        # criterion_kd = nn.KLDivLoss(reduction='batchmean') # Example KL divergence loss\n",
        "        # print(\"KL Divergence loss for knowledge distillation considered.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Model instance not found. Ensure the model definition step was successful.\")\n",
        "        # Cannot proceed without the model.\n",
        "        # This will lead to a failure in completing the subtask.\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}. Please ensure the data directory structure for Stage 2 train data is correct.\")\n",
        "    # Cannot proceed without the data loaders being successfully instantiated.\n",
        "    # This will lead to a failure in completing the subtask.\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}. Please ensure there are class subdirectories in your Stage 2 train data folder.\")\n",
        "    # Cannot proceed without valid data.\n",
        "    # This will lead to a failure in completing the subtask.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    # Catch any other unexpected errors.\n",
        "    # This will lead to a failure in completing the subtask.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Directory not found: /content/sample_data/stage_2/train. Please ensure the data directory structure for Stage 2 train data is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5585af2"
      },
      "source": [
        "## 増分学習\n",
        "\n",
        "### Subtask:\n",
        "Stage 1で学習したモデルをベースに、Stage 2のデータを使用して増分学習を行います。知識蒸留を導入する場合は、この段階で適用します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a103225f"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the instruction to perform incremental learning using Stage 2 data and the history of failures due to data loading issues, the first crucial step is to attempt to load the Stage 2 training data again. I will re-use the existing `BarkDataset` and `DataLoader` definitions (assuming they are available from previous successful code cells) and attempt to instantiate the `stage2_train_loader`. If successful, I will then proceed to load the previously saved Stage 1 model, prepare it for incremental learning, define the optimizer and loss function, and execute the incremental training loop as outlined in the instructions. Knowledge distillation will be incorporated if a teacher model is available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6fc0705",
        "outputId": "b00368e6-2d52-48e0-8829-25bad1a9391a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import os\n",
        "import torch.nn.functional as F # For KL Divergence if using KD\n",
        "\n",
        "# Re-using the dataset and dataloader definitions from previous steps.\n",
        "# If the previous definition cell failed, this will fail again.\n",
        "# Assuming base_data_dir is defined, but confirming its existence is crucial.\n",
        "\n",
        "# Let's explicitly check if the necessary data loader definition was successful previously\n",
        "# by attempting to access the variable name. If it doesn't exist, we'll know the data loading failed.\n",
        "try:\n",
        "    # Attempt to access stage2_train_loader to see if it was successfully created before\n",
        "    _ = stage2_train_loader\n",
        "    print(\"stage2_train_loader found from previous steps.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"stage2_train_loader not found. Attempting to re-create DataLoaders.\")\n",
        "    # If the loader was not found, it means the previous data loading steps failed.\n",
        "    # We need to re-attempt defining the dataset and dataloader here.\n",
        "    # This assumes the BarkDataset and transformation definitions are available globally from previous successful cells.\n",
        "\n",
        "    # Using the placeholder base_data_dir again. This needs to be correct for the code to run.\n",
        "    base_data_dir = '/content/sample_data' # Placeholder, replace with actual path\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        stage2_train_dataset = BarkDataset(base_data_dir, stage=2, split='train', transform=transform)\n",
        "        stage2_train_loader = DataLoader(stage2_train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "        print(\"Stage 2 train dataset and loader successfully created.\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error re-creating Stage 2 data loader: {e}. Please ensure the data directory structure for Stage 2 train data is correct.\")\n",
        "        # Cannot proceed without the data loaders being successfully instantiated.\n",
        "        # Signal a critical error and potentially exit or finish with failure later.\n",
        "        stage2_train_loader = None # Ensure the variable is None if creation fails\n",
        "    except ValueError as e:\n",
        "        print(f\"Error re-creating Stage 2 data loader: {e}. Please ensure there are class subdirectories in your Stage 2 train data folder.\")\n",
        "        stage2_train_loader = None # Ensure the variable is None if creation fails\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Stage 2 data loader re-creation: {e}\")\n",
        "        stage2_train_loader = None # Ensure the variable is None if creation fails\n",
        "\n",
        "\n",
        "# Proceed with incremental learning only if the data loader is available\n",
        "if stage2_train_loader is not None:\n",
        "    # Check if the model exists from previous steps\n",
        "    if 'model' in locals():\n",
        "        print(\"Model instance found.\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device) # Ensure model is on the correct device\n",
        "\n",
        "        # Load the best Stage 1 model weights if saved\n",
        "        stage1_model_path = 'best_stage1_model.pth'\n",
        "        if os.path.exists(stage1_model_path):\n",
        "            print(f\"Loading Stage 1 model weights from {stage1_model_path}\")\n",
        "            try:\n",
        "                model.load_state_dict(torch.load(stage1_model_path, map_location=device))\n",
        "                print(\"Stage 1 model weights loaded successfully.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading Stage 1 model weights: {e}\")\n",
        "                # Continue with the potentially untrained model or handle as failure\n",
        "        else:\n",
        "            print(f\"Warning: {stage1_model_path} not found. Starting incremental learning with potentially untrained or partially trained model.\")\n",
        "\n",
        "\n",
        "        # 3. Set Stage 1 model as teacher model if knowledge distillation is used\n",
        "        # Create a separate instance for the teacher if we need to preserve its state.\n",
        "        # For simplicity, we'll just use the 'model' instance and ensure it's in eval mode when used as teacher.\n",
        "        teacher_model = model # Using the same model instance as teacher\n",
        "        teacher_model.eval() # Set teacher model to evaluation mode\n",
        "        print(\"Stage 1 model set as teacher model (in eval mode).\")\n",
        "\n",
        "        # Determine the number of classes for Stage 2.\n",
        "        # This requires the stage2_train_dataset to be successfully instantiated.\n",
        "        # If Stage 2 has new classes, the model's final layer needs modification.\n",
        "        num_classes_stage2 = len(stage2_train_dataset.classes)\n",
        "        print(f\"Number of classes in Stage 2 train data: {num_classes_stage2}\")\n",
        "\n",
        "        # Important: If Stage 2 introduces new classes, the output layer of the model needs to be adapted.\n",
        "        # This is a crucial part of incremental learning for class-incremental scenarios.\n",
        "        # Assuming for now that Stage 2 either adds new classes or uses existing ones.\n",
        "        # If new classes are added, the final layer needs to be reinitialized or expanded.\n",
        "\n",
        "        # Simple approach: Replace the final layer if the number of classes changes\n",
        "        # This might require re-training the new layer from scratch or with a low learning rate.\n",
        "        # A more sophisticated approach might involve adding a new head or using techniques like freezing layers.\n",
        "        if model.fc2.out_features != num_classes_stage2:\n",
        "             print(f\"Adapting model output layer from {model.fc2.out_features} to {num_classes_stage2} classes.\")\n",
        "             # Create a new fully connected layer\n",
        "             new_fc = nn.Linear(model.fc2.in_features, num_classes_stage2).to(device)\n",
        "             # Replace the old final layer\n",
        "             model.fc2 = new_fc\n",
        "             print(\"Model output layer adapted.\")\n",
        "             # Note: Parameters of the new layer are randomly initialized by default.\n",
        "             # You might want to initialize them differently or train them specifically.\n",
        "\n",
        "        # 4. Define a new optimizer for incremental learning (Stage 2)\n",
        "        # Using Adam with a potentially different learning rate, optimizing all model parameters.\n",
        "        # If specific layers were frozen or modified, you might want to optimize only the modified/unfrozen ones.\n",
        "        incremental_optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "        print(\"Optimizer for incremental learning defined.\")\n",
        "\n",
        "        # 5. Define loss function(s)\n",
        "        # Cross-entropy loss for the student model\n",
        "        criterion_ce = nn.CrossEntropyLoss()\n",
        "        print(\"Cross-entropy loss defined.\")\n",
        "\n",
        "        # Define KL divergence loss for knowledge distillation\n",
        "        # Temperature parameter for softening the teacher's output\n",
        "        temperature = 2.0 # Example temperature value\n",
        "        criterion_kd = nn.KLDivLoss(reduction='batchmean')\n",
        "        print(f\"KL Divergence loss for knowledge distillation defined with temperature {temperature}.\")\n",
        "\n",
        "        # Define alpha weight for balancing CE and KD loss\n",
        "        alpha = 0.5 # Example weight for KD loss (0.0 means only CE, 1.0 means only KD)\n",
        "        print(f\"Alpha weight for knowledge distillation set to {alpha}.\")\n",
        "\n",
        "\n",
        "        # 6. Set up training parameters for incremental learning\n",
        "        num_epochs_incremental = 5 # Number of epochs for incremental training\n",
        "        print(f\"Starting incremental training for {num_epochs_incremental} epochs...\")\n",
        "\n",
        "        # Incremental Training loop\n",
        "        for epoch in range(num_epochs_incremental):\n",
        "            model.train()  # Set model to training mode\n",
        "            running_loss_incremental = 0.0\n",
        "            correct_train_incremental = 0\n",
        "            total_train_incremental = 0\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            for i, data in enumerate(stage2_train_loader):\n",
        "                inputs, labels = data\n",
        "                # Filter out None values in case of image loading errors\n",
        "                if inputs is None or labels is None:\n",
        "                    continue\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                incremental_optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass - get student output\n",
        "                student_outputs = model(inputs)\n",
        "\n",
        "                # Calculate Cross-Entropy Loss\n",
        "                ce_loss = criterion_ce(student_outputs, labels)\n",
        "\n",
        "                # Calculate Knowledge Distillation Loss\n",
        "                # Get teacher output (no gradients needed for teacher)\n",
        "                with torch.no_grad():\n",
        "                    teacher_outputs = teacher_model(inputs)\n",
        "\n",
        "                # Apply temperature and softmax to both student and teacher outputs\n",
        "                student_outputs_soft = F.log_softmax(student_outputs / temperature, dim=1)\n",
        "                teacher_outputs_soft = F.softmax(teacher_outputs / temperature, dim=1)\n",
        "\n",
        "                # Calculate KL Divergence loss\n",
        "                kd_loss = criterion_kd(student_outputs_soft, teacher_outputs_soft)\n",
        "\n",
        "                # Combine losses\n",
        "                # Total loss = (1 - alpha) * CE_loss + alpha * KD_loss * temperature^2\n",
        "                # The temperature^2 scaling is often applied to the KD loss when using KLDivLoss\n",
        "                # with log_softmax on the student output and softmax on the teacher output.\n",
        "                loss = (1 - alpha) * ce_loss + alpha * kd_loss * (temperature ** 2)\n",
        "\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                incremental_optimizer.step()\n",
        "\n",
        "                running_loss_incremental += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(student_outputs.data, 1)\n",
        "                total_train_incremental += labels.size(0)\n",
        "                correct_train_incremental += (predicted == labels).sum().item()\n",
        "\n",
        "            epoch_loss_incremental = running_loss_incremental / len(stage2_train_loader)\n",
        "            epoch_accuracy_incremental = 100 * correct_train_incremental / total_train_incremental\n",
        "            epoch_time = time.time() - start_time\n",
        "\n",
        "            print(f'Incremental Epoch [{epoch+1}/{num_epochs_incremental}], Loss: {epoch_loss_incremental:.4f}, Accuracy: {epoch_accuracy_incremental:.2f}%, Time: {epoch_time:.2f}s')\n",
        "\n",
        "            # Optional: Add validation step here using stage2_val_loader if available\n",
        "            # and save the model if performance improves.\n",
        "            # This would require checking if stage2_val_loader was successfully created.\n",
        "\n",
        "\n",
        "        print(\"Finished Incremental Training.\")\n",
        "\n",
        "        # Save the incrementally trained model\n",
        "        torch.save(model.state_dict(), 'incrementally_trained_model_stage2.pth')\n",
        "        print(\"Saved incrementally trained model state dictionary.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Cannot perform incremental training: Model instance not found.\")\n",
        "        # If model is not found, the previous model definition step failed.\n",
        "\n",
        "else:\n",
        "    print(\"Cannot perform incremental training: Stage 2 train data loader not available.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stage2_train_loader not found. Attempting to re-create DataLoaders.\n",
            "Error re-creating Stage 2 data loader: Directory not found: /content/sample_data/stage_2/train. Please ensure the data directory structure for Stage 2 train data is correct.\n",
            "Cannot perform incremental training: Stage 2 train data loader not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8b06367"
      },
      "source": [
        "## 総合評価\n",
        "\n",
        "### Subtask:\n",
        "増分学習後のモデルを全てのクラスを含むテストデータ (`test_all`) で評価し、モデル全体の性能を確認します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370b6c82"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the incrementally trained model on the test data containing all classes. This involves setting the model to evaluation mode, iterating through the all-classes test loader, making predictions, and calculating evaluation metrics like accuracy and confusion matrix. The code will check if the model and the all-classes test loader exist from previous steps and handle potential file loading errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c986dd8e",
        "outputId": "7d256dfc-35e1-4214-dbd4-575af9955bcf"
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Check if model and all_classes_test_loader exist from previous steps.\n",
        "# If previous steps failed due to data loading, these might not be defined or the model might be untrained.\n",
        "# We need to gracefully handle these cases.\n",
        "\n",
        "if 'model' not in locals():\n",
        "    print(\"Error: Model instance not found. Ensure the model definition step was successful.\")\n",
        "    # If the model doesn't exist, we cannot proceed with evaluation.\n",
        "    evaluation_successful = False\n",
        "elif 'all_classes_test_loader' not in locals() or all_classes_test_loader is None:\n",
        "     print(\"Error: all_classes_test_loader not found or not successfully created. Ensure data loading for all classes test data was successful.\")\n",
        "     # If the data loader doesn't exist or failed creation, we cannot proceed.\n",
        "     evaluation_successful = False\n",
        "else:\n",
        "    evaluation_successful = True\n",
        "    print(\"Model instance and all_classes_test_loader found.\")\n",
        "\n",
        "    # Proceed with evaluation only if prerequisites are met\n",
        "    if evaluation_successful:\n",
        "        # Set model to evaluation mode\n",
        "        model.eval()\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device) # Ensure model is on the correct device\n",
        "\n",
        "        # Load the incrementally trained model weights if saved\n",
        "        incremental_model_path = 'incrementally_trained_model_stage2.pth'\n",
        "        if os.path.exists(incremental_model_path):\n",
        "            print(f\"Loading incrementally trained model weights from {incremental_model_path}\")\n",
        "            try:\n",
        "                model.load_state_dict(torch.load(incremental_model_path, map_location=device))\n",
        "                print(\"Incrementally trained model weights loaded successfully.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading incrementally trained model weights: {e}\")\n",
        "                print(\"Proceeding with the current state of the model, which might be Stage 1 trained or untrained.\")\n",
        "        else:\n",
        "            print(f\"Warning: {incremental_model_path} not found. Evaluating the current state of the model (likely Stage 1 trained or untrained).\")\n",
        "\n",
        "\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        print(\"Starting evaluation on all classes test data...\")\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        with torch.no_grad():\n",
        "            for data in all_classes_test_loader:\n",
        "                images, labels = data\n",
        "                # Filter out None values in case of image loading errors\n",
        "                if images is None or labels is None:\n",
        "                    continue\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        if len(all_labels) > 0:\n",
        "            accuracy = accuracy_score(all_labels, all_predictions)\n",
        "            conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "            print(f'Overall Test Accuracy (All Classes): {accuracy:.4f}')\n",
        "            print('Overall Confusion Matrix (All Classes):')\n",
        "            print(conf_matrix)\n",
        "\n",
        "            # Determine class names if possible (assuming all_classes_test_dataset was created successfully)\n",
        "            if 'all_classes_test_dataset' in locals() and all_classes_test_dataset is not None:\n",
        "                print('All Classes:')\n",
        "                print(all_classes_test_dataset.classes)\n",
        "            else:\n",
        "                print(\"Could not retrieve class names from the dataset.\")\n",
        "\n",
        "        else:\n",
        "            print(\"No data available for evaluation. Ensure all_classes_test_loader is populated.\")\n",
        "\n",
        "\n",
        "print(\"Finished evaluation attempt.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: all_classes_test_loader not found or not successfully created. Ensure data loading for all classes test data was successful.\n",
            "Finished evaluation attempt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2d821e4"
      },
      "source": [
        "## 結果の分析と考察\n",
        "\n",
        "### Subtask:\n",
        "学習過程、評価結果を分析し、増分学習や知識蒸留の効果、モデルの課題などを考察します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d22f98ab"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The entire data analysis process failed at multiple steps due to a `FileNotFoundError`.\n",
        "*   Specifically, attempts to instantiate `BarkDataset` and `AllClassesTestDataset` for Stage 1, Stage 2, and all-classes test data failed because the base directory `/content/sample_data` did not contain the expected subdirectories (`stage_1`, `stage_2`, `all_classes_test`) with the required data structure.\n",
        "*   As a consequence of the data loading failures, the subsequent steps for model training (Stage 1 and incremental Stage 2) and evaluation (Stage 1 test and all-classes test) could not be executed.\n",
        "*   While the code for defining the model, optimizer, loss functions (including KL Divergence for potential knowledge distillation), and the training/evaluation loops was written, its functionality could not be verified due to the lack of data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Critical Next Step:** The absolute necessary next step is to ensure the data is correctly placed at the specified `base_data_dir` (`/content/sample_data` or an updated path) following the expected directory structure (`stage_1/[train|val|test]/class_name/`, `stage_2/[train|val|test]/class_name/`, `all_classes_test/class_name/`).\n",
        "*   Once the data is correctly set up, re-run the entire process sequentially to confirm successful data loading, model training on Stage 1, evaluation on Stage 1, incremental training on Stage 2 (with or without knowledge distillation), and finally, comprehensive evaluation on the all-classes test set.\n"
      ]
    }
  ]
}